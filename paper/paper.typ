#import "template.typ": *
#import "@preview/cetz:0.3.1": canvas, draw

#show: report.with(
  title: [ Actor Free critic Update for Off-policy and Offline learning ],
  course: [ #smallcaps[Ai2d] project ],
  authors: ("Paul Chambaz", "Frédéric Li Combeau" ),
  university: [ Sorbonne University ],
  reference: [ #smallcaps[Ai2d] & #smallcaps[Iq] M1 ],
  bibliography-path: "bibliography.yml",
  nb-columns: 2,
  abstract: [
    #lorem(20)
  ]
)

== Introduction

== Related Work

Reinforcement learning algorithms for continuous control domains have evolved
significantly in recent years. The development of actor-critic architectures
has proven particularly effective, with algorithms like DDPG @lillicrap2019 and
SAC @haarnoja2018 establishing themselves as standard approaches. These methods
combine the advantages of policy gradient methods with value-based learning,
enabling sample-efficient learning in continuous action spaces.

DDPG employs a deterministic actor that maximizes a learned Q-function,
operating entirely off-policy to improve sample efficiency. However, DDPG
presents stability issues, demonstrating sensitivity to hyperparameter choices
and exploration strategies. SAC extends this framework by incorporating entropy
maximization, encouraging exploration while learning a stochastic policy. The
entropy term provides additional stability, allowing SAC to achieve both better
sample efficiency and final performance compared to DDPG in many domains.

The challenge of addressing the distribution shift when transitioning from
offline to online reinforcement learning has received increasing attention.
Offline reinforcement learning methods train agents using previously collected
datasets without environment interaction, eliminating exploration costs but
introducing optimization difficulties. IQL @kostrikov2021 approaches this
problem by completely avoiding direct querying of the learned Q-function with
unseen actions during training, using expectile regression to estimate the
maximum value of Q-functions. Similarly, CALQL @nakamoto2024 applies a
constraint to the conservative Q-learning framework to reduce overestimation
bias during offline learning while enabling efficient online fine-tuning. Both
methods aim to mitigate performance degradation observed when agents trained
offline begin interacting with environments directly.

The ability to learn truly off-policy—from data generated by arbitrary or random policies—represents another research direction. Traditional actor-critic methods are categorized as off-policy but often struggle when presented with data significantly different from their current policy distribution. This limitation arises because their critic updates depend on actions sampled by the actor, creating an implicit coupling that restricts genuine off-policy learning. Some algorithms have attempted to address this issue through several methods but the fundamental actor-critic interdependence remains.

AFU @perringilbert2024 introduces a structural departure from previous approaches by maintaining critic updates that remain entirely independent from the actor. Unlike other algorithms derived from Q-learning for continuous control, AFU aims to solve the maximization problem inherent in Q-learning through a mechanism based on value and advantage decomposition, employing conditional gradient scaling. This approach potentially enables more effective learning from arbitrary data distributions without requiring explicit constraints during the critic learning phase.

== Preleminaries

We consider a discounted infinite horizon Markov Decision Problem (MDP) defined
as a tuple $<S, A, T, R, gamma>$, where $S$ represents the state space, $A$
denotes a continuous action space, $T$ is a stochastic transition function, $R:
S times A -> RR$ is a reward function, and $0 <= gamma < 1$ is a discount
factor. When an agent performs an action $a in A$ in state $s in S$, it
transitions to a new state $s'$ according to the transition probability
$T(s'|s,a)$ and receives a reward $r = R(s,a)$. We denote transitions as tuples
$(s, a, r, s')$.

The goal in reinforcement learning is to find a policy $pi: S -> A$
that maximizes the expected sum of discounted rewards. The optimal Q-function
$Q^*$ is defined as:

$
Q^* (s, a) = EE [
  sum_(t=0)^oo gamma^t R(s_t, a_t) | s_0 = s, a_0 = a, pi^*
]
$

where the policy used from $t = 1$ onwards is $pi^*$, which selects actions
optimally in every state. The optimal value function $V^*$ satisfies $V^*(s) =
max_{a in A} (Q^*(s, a))$.

In deep reinforcement learning, we approximate the value functions using neural
networks. We denote by $V_phi$ a function approximators for the value function,
and by $Q_psi$ a function approximator for the Q-function (the critic), where
$phi$, and $psi$ are the parameter vectors of neural networks.

On-policy and off-policy learning represent two distinct approaches in
reinforcement learning. In on-policy learning, the agent learns from data
collected using its current policy. In contrast, off-policy learning allows the
agent to learn from data collected using any policy, including random
exploration or previously stored experiences. This distinction is crucial as
truly off-policy algorithms can potentially learn more efficiently by reusing
diverse experiences.

Offline reinforcement learning takes the off-policy concept further by learning
entirely from a fixed dataset of previously collected transitions without any
environment interaction during training. When transitioning from offline to
online learning (where the agent begins to interact with the environment),
algorithms often suffer from distribution shift problems as the learned policy
encounters states and actions not represented in the offline dataset.

Actor-critic methods combine policy-based and value-based approaches. The actor
(policy network) determines which actions to take, while the critic (value
network) evaluates these actions. In traditional actor-critic architectures
like SAC and DDPG, the critic's updates depend on actions sampled by the actor,
creating an interdependence between the two components. The AFU algorithm
represents a departure from this approach by updating the critic independently
of the actor.

The temporal difference (TD) learning used in many algorithms aims to minimize
the following loss function for the critic:

$
L_Q (psi) = EE_((s, a, r, s') ~ B) [
  (Q_psi (s, a) - r - gamma V_phi (s'))^2
]
$

where $B$ represents a mini-batch of transitions sampled from an experience
replay buffer.

== Methods

=== Environment
For our experiments, we utilized the _Gymnasium_ framework, a widely adopted
benchmark for reinforcement learning research. Gymnasium provides standardized
environments, allowing for reproducible evaluation of algorithmic performance.
We primarily focused on continuous control tasks including Pendulum, CartPole
(continuous version), LunarLander (continuous version) and MountainCar
(continuous version).

To facilitate our research on off-policy learning properties, we extended these
environments with additional functionality. Each environment was modified to
support direct state manipulation through the implementation of a
`_set_state()` method. This modification enables precise control over the
system state, allowing us to sample uniformly from the state-action space
during our off-policy learning experiments. It should be noted that such direct
state manipulation represents a research tool rather than a practical
capability in real-world scenarios, where complete state control is rarely
possible.

=== Algorithms
We implemented several state-of-the-art deep reinforcement learning algorithms
using PyTorch and the BBRL (BlackBoard Reinforcement Learning) framework. BBRL
provides a modular architecture where agents interact through a shared
workspace, facilitating the implementation of complex algorithms with clear
separation of components. Each algorithm implementation follows the same
structure, comprising neural network architectures, training procedures, and
evaluation methods.

==== DDPG
Deep Deterministic Policy Gradient combines the deterministic policy gradient
algorithm with deep neural networks. It consists of two primary networks. The
actor network implements a deterministic policy $pi(s)$ that maps states to
specific actions. The critic network evaluates the quality of state-action
pairs by approximating the Q-function $Q(s, a)$. For stable learning, DDPG
employs target networks for both actor and critic, which are updated using soft
updates: $theta^"target" <- tau theta + (1 - tau) theta^"target"$, where $tau$
is a small value controlling the update rate. DDPG also uses a replay buffer to
store and randomly sample transitions, breaking the correlation between
consecutive samples and stabilizing learning.

==== SAC
Soft Actor-Critic extends the actor-critic architecture by incorporating
entropy maximization, encouraging exploration through stochastic policies. Our
SAC implementation includes a policy network, a Q-network and a V-network.
Unlike DDPG's deterministic policy, SAC's policy is stochastic, outputting a
Gaussian distribution over actions. The network produces both the mean $mu(s)$
and log standard deviation $log(sigma(s))$ of this distribution. Actions are
sampled using the reparameterization trick and squashed through a tanh function
to bound them. SAC employs two Q-networks to mitigate overestimation bias, a
common issue in Q-learning. Both networks approximate $Q(s, a)$ and the minimum
of their predictions is used for updates. The value network estimates the state
value $V(s)$, which represents the expected future return starting from state $s$
when following the policy. SAC optimizes the expected return plus the entropy
of the policy using a temperature parameter that determines the relative
importance of entropy versus reward. This parameter is automatically adjusted
during training to achieve a target entropy.

==== AFU
The Actor-Free Updates algorithm represents a significant departure from
traditional actor-critic methods. While algorithms like DDPG and SAC update
their critic using actions generated by the actor, AFU decouples these
components, enabling critic updates that are truly independent of the actor. 
AFU's architecture consists of two value networks, two advantage networks, a
Q-network and a policy network. The advantage networks estimates the the
advantage function $A(s, a)$, which represents how much better taking action
$a$ in state $s$ is compared to the average action.

The key innovation in AFU lies in how it computes the value and advantage
functions. Instead of relying on the actor to estimate the maximum value of
$Q(s, a)$ over actions, AFU directly decomposes $Q(s, a)$ into $V(s) + A(s,
a)$. This decomposition, combined with a conditional gradient scaling
mechanism, allows AFU to solve the maximization problem in Q-learning without
depending on the actor. This mechanism allows AFU to maintain stable learning
by adaptively adjusting the gradient flow depending on whether the current
estimate falls short of the target.

==== IQL
Implicit Q-Learning approaches offline reinforcement learning by avoiding
direct querying of the Q-function with unseen actions during training. This
algorithm uses expectile regression to estimate the maximum value of
Q-functions without requiring access to the optimal action. IQL operates on
three main components: a value network, two Q-networks to reduce overestimation
bias, and a policy network. The value function is trained using expectile
regression at a specific quantile $tau$, enabling it to approximate the maximum
of the Q-function distribution. The policy is then extracted through
advantage-weighted regression, allowing it to favor actions with higher
advantages while remaining within the data distribution. This approach
effectively addresses the distribution shift problem in offline RL by never
evaluating actions outside the training dataset, making it well-suited for
purely offline learning scenarios but potentially limiting its adaptability
during transitions to online learning.

==== Cal-QL
Calibrated Q-Learning builds upon conservative Q-learning approaches for
offline reinforcement learning while enabling smoother transition to online
learning. CALQL employs a Q-network, a target network, and a policy network
similar to SAC's stochastic policy. What distinguishes CALQL is its use of
reference values precomputed from the dataset using Monte Carlo returns, which
serve as a calibration mechanism. The learning objective combines a standard
temporal difference error with a conservative regularization term that
explicitly penalizes out-of-distribution actions. This calibration helps
prevent both underestimation and overestimation of Q-values, addressing a key
challenge in offline-to-online transition where traditional conservative
approaches tend to be overly pessimistic. CALQL's balanced approach allows it
to maintain sufficient conservatism during offline learning while facilitating
effective exploration when transitioning to online learning.

==== Off-policy learning experimental setup
Our first experiment aims to evaluate whether AFU can truly learn from randomly
generated data, a capability that would distinguish it from traditional
off-policy algorithms like DDPG and SAC. The hypothesis stems from the
theoretical properties of AFU's critic update mechanism, which, unlike
SARSA-based algorithms, does not rely on the actor's improvement to approximate
the max-Q operation.

Traditional actor-critic algorithms like SAC and DDPG are structurally closer
to SARSA than to Q-learning. Despite being categorized as off-policy, they
depend on the actor to generate actions for the critic's updates, creating an
implicit coupling. In contrast, AFU's value and advantage decomposition,
combined with conditional gradient scaling, allows it to compute critic updates
independently of the actor, potentially enabling true off-policy learning.

To test this hypothesis, we designed a protocol where instead of collecting
experience through environment interaction with the current policy, we randomly
sample states from the observation space and actions from the action space.
This sampling is uniform, representing the extreme case of off-policy data with
no correlation to any learning policy. If AFU can converge under these
conditions while traditional algorithms struggle, it would validate its
theoretical advantage in truly off-policy learning.

==== Offline-to-online transition experimental setup
Our second experiment investigates the stability of algorithms during the
transition from offline to online learning. This transition presents a
significant challenge due to the distribution shift between the fixed offline
dataset and the data generated by the current policy during online learning.

Offline reinforcement learning algorithms often employ conservatism to prevent
overestimation of out-of-distribution actions. While this conservatism is
beneficial during offline learning, it can hinder exploration during subsequent
online learning. Algorithms like IQL and CALQL address this through different
mechanisms: IQL uses expectile regression to estimate maximum values without
querying unseen actions, while CALQL explicitly constrains Q-values for
out-of-distribution actions.

We hypothesize that AFU may exhibit superior stability during this transition
due to its ability to estimate maximum Q-values without relying on the actor.
This capability potentially reduces the need for explicit conservatism,
allowing AFU to adapt more smoothly when transitioning to online learning.

To test this hypothesis, we generated datasets from policies at different
stages of training, capturing trajectories at fixed intervals during policy
learning. We then trained algorithms offline on these datasets before
transitioning to online learning. The primary evaluation metric is the area
between the learning curve and the maximum stable performance level following
the transition point, which quantifies how quickly and stably an algorithm
recovers during the online phase.








==== First Results

We also evaluated AFU in an Offline-to-Online setting, where the agent first learns from a fixed dataset and then transitions to online learning. We used the same datasets as in the previous experiments, but this time we trained AFU for 1 million steps in an offline setting before switching to online training.

== Conclusion

== Appendix A

=== Experimental Details

The custom environment wrappers used in this study include the following methods:
- `_set_state`: Allows resetting the environment to a specific state.
- `get_obs`: Retrieves the current observation.
- `get_observation_space`: Returns the observation space of the environment.
- `get_action_space`: Returns the action space of the environment.

These wrappers were implemented to ensure compatibility with the experiments.

=== Hyperparameters

The following hyperparameters were used for the experiments:

- Hidden size : $256$
- Learning rate: $3 times 10^(-4)$
- Discount factor ($gamma$): $0.99$
- Batch size: $256$
- Replay buffer size: $200000$
- Target network update rate ($tau$): $0.01$
- Gradient reduction (for AFU only) : $0.5$
- Noise standard deviation (for DDPG only) : $0.1$
- Expectile regression ($tau'$) (for IQL only): $0.9$
- Inverse temperature ($beta$) (for IQL only): $3.0$
- Conservative factor ($alpha$) (for Cal-QL only): $5$
- Random repetitions ($n$) (for Cal-QL only): $4$

=== Reproducibility

The code for all algorithms and experiments is available at #link("https://github.com/paulchambaz/afu-study"). The repository includes detailed instructions for setting up the environment and running the experiments. The code is licensed under the GPLv3 license and freely available.

== Appendix B

=== Implementation Notes on CAL-QL

In our review of the CALQL implementation presented in the original paper, we
identified two minor inconsistencies in the provided pseudocode that we
corrected in our implementation:

1. In the target Q-value calculation, the original code shows:

```
target_qval = target_critic(batch['observations'], next_pi_actions)
```

However, this should use the next observations according to the Bellman equation:

```
target_qval = target_critic(batch['next_observations'], next_pi_actions)
```

2. In the random actions Q-value calculation, the original code indicates:

```
q_rand_is = critic(batch['observations'], random_actions) - random_pi
```

For consistency with the log-probabilities used elsewhere in the algorithm,
this should instead use the logarithm of the uniform density:

```
q_rand_is = critic(batch['observations'], random_actions) - log(random_pi)
```

These corrections align with both the mathematical principles of the algorithm
and the authors' actual implementation in their source code repository. While
these inconsistencies are minor and likely typographical in nature, they are
worth noting for those aiming to implement CALQL correctly.

=== Implementation Notes on IQL

In our review of the Implicit Q-Learning implementation presented in the
original paper, we identified a minor inconsistency in the provided formula
that we corrected in our implementation:

The policy loss function in Equation (7) of the paper shows:

$
L_pi (phi.alt) = EE_((s,a)~D) \
[ exp(beta (Q_theta (s, a) - V_psi (s))) log pi_phi.alt (a | s) ]
$

However, since this is a loss function that should be minimized during optimization, the correct formula should include a negative sign:

$
L_pi (phi.alt) = EE_((s,a)~D) \
[ exp(beta (Q_theta (s, a) - V_psi (s))) log pi_phi.alt (a | s) ]
$

This correction aligns with the mathematical principles of advantage-weighted
regression, where the objective is to maximize the likelihood of actions with
high advantages. While this inconsistency is minor and likely typographical in
nature, it's worth noting for those aiming to implement IQL correctly.

== Appendix C

=== Implementation detail of DDPG

DDPG optimizes a continuous control policy using deterministic actions and off-policy learning. It uses two neural networks with parameters $theta$ and $phi$: a critic (Q-network) and an actor (policy network). The critic $Q_theta (s, a)$ estimates expected returns for state-action pairs, while the actor $mu_phi (s)$ maps states directly to deterministic actions. Target networks with parameters $theta'$ and $phi'$ stabilize learning through soft updates with rate $tau$. Exploration during training uses Gaussian noise $cal(N) (0, sigma^2)$ with standard deviation $sigma$. Experiences $(s, a, r, s')$ are stored in a replay buffer $cal(B)$ of size $N$, from which we sample mini-batches of size $B$ for updates.


The first is a Q-network, which does a forward pass on a state-action pair and estimates the expected reward that the agent gets after doing a given action in a given state. To update its weights, we compute the following loss:

$
L_Q (theta) = EE_((s, a, r, s')~ B) \
[ (Q_theta (s, a) - (r + gamma Q_(theta') (s', mu_(phi')(s'))))^2 ]
$

The second is a policy network (Actor), which outputs deterministic actions for any given state. Unlike SAC's stochastic policy, it directly maps states to optimal actions without probability distributions. Using a tanh activation to bound actions within [-1, 1], the policy is optimized by maximizing the expected Q-value:

$
L_(mu) (phi) = -EE_(s ~ B) [ Q_(theta) (s, mu_(phi) (s)) ]
$

=== Implementation detail of SAC

SAC implements an actor-critic method with entropy regularization for improved exploration. It uses five neural networks: twin critics with parameters $theta_1$ and $theta_2$ to reduce overestimation bias, a value network with parameters $psi$, a target value network with parameters $psi'$, and a policy network with parameters $phi$. The temperature parameter $alpha$ controls exploration, with target entropy set to $-|cal(A)|$ where $cal(A)$ is the action space. Experience tuples $(s, a, r, s')$ are stored in a replay buffer $cal(B)$ of capacity $N$, from which mini-batches of size $B$ are sampled. Networks are updated using learning rates $l_Q$, $l_V$, $l_pi$, and $l_alpha$ for critics, value network, policy, and temperature respectively, with target networks soft-updated at rate $tau$.

The first is a V-network, which does a forward pass on a given state and estimates the expected reward an agent can gain in the state. To update its weights, we compute the following loss:

$
L_V (psi) = EE_((s, a, r, s')~B) \ [ (V_psi (s) -
(min Q_theta_i (s, a_s) - alpha log(pi_phi (a_s | s))))^2 ]
$

where $a_s ~ pi_phi (.|s)$ is an action sampled from the policy.

The second is a Q-network, which does a forward pass on a state-action pair and estimates the expected reward that the agent gets after doing a given action in a given state. To update its weights, we compute the following loss:

$
L_Q (theta_i) = EE_((s, a, r, s')~B) \
[ (Q_(theta_i) (s, a) - r - gamma V_(psi^"target") (s'))^2 ]
$

The last is a policy network, which parameterizes a Gaussian distribution over
actions. It outputs mean actions and learns log standard deviations as
parameters. Using the reparameterization trick and tanh squashing for bounded
actions, the policy is optimized by minimizing:

$
L_pi (phi.alt) = EE [ alpha log(pi_(phi.alt) (a_s | s)) - Q_(theta_i) (s, a_s) ]
$

=== Implementation detail of AFU
Actor Free critic Updates implements an actor-critic architecture with a novel advantage decomposition approach. It uses six neural networks: a main critic network with parameters $psi$, twin value networks with parameters $phi_1$ and $phi_2$, their target versions with parameters $phi_1'$ and $phi_2'$, twin advantage networks with parameters $xi_1$ and $xi_2$, and a policy network with parameters $theta$. The temperature parameter $alpha$ controls exploration, with target entropy set to $-|cal(A)|$ where $cal(A)$ is the action space. The gradient reduction parameter $rho in [0, 1]$ controls value function learning dynamics. Experiences $(s, a, r, s')$ are stored in a replay buffer $cal(B)$ of capacity $N$, from which mini-batches of size $B$ are sampled. Networks are updated using learning rates $l_Q$, $l_V$, $l_\pi$, and $l_alpha$ for critic, value/advantage networks, policy, and temperature respectively, with target networks soft-updated at rate $tau$.

The first is a Q-network, which does a forward pass on a state-action pair and estimates the expected reward of an agent taking action $a$ in state $s$. To update its weights, we compute the following loss:

$
L_Q (psi) = EE_((s, a, r, s') ~ cal(B)) \
[ (Q_psi (s, a) - r - gamma min_(i in {1,2}) V_(phi_i') (s'))^2 ]
$

The second is a combined value and advantage loss for each network pair, which updates both value and advantage networks based on the relation $Q(s,a) = V(s) + A(s,a)$. To update their weights, we compute:

$
L_("VA") (phi_i, xi_i) = EE_((s, a, r, s') ~ cal(B)) \
[ Z(Upsilon_i^a (s) - r - gamma min_(i in {1,2}) V_(phi_i') (s'), A_(xi_i) (s,a)) ]
$

where:

$
Z(x, y) = cases((x + y)^2 quad "if" x <= 0, x^2 + y^2 quad "otherwise") \
Upsilon_i^a (s) = (1 - rho . I_i (s,a)) V_(phi_i) (s) + rho . I_i (s,a) . V_(phi_i)^("nograd") (s) \
I_i (s, a) = cases(1 quad "if" V_(phi_i) (s) + A_(xi_i) (s, a) < Q_psi (s, a), 0 quad "otherwise")
$

The last is a policy network that parameterizes a Gaussian distribution over actions. It outputs mean actions and learns log standard deviations as parameters. Using the reparameterization trick and tanh squashing for bounded actions, the policy is optimized by minimizing:

$
L_pi (theta) = EE_(s ~ cal(B)) [ alpha log(pi_theta (a_s | s)) - Q_psi (s, a_s) ]
$

=== Implementation detail of IQL
Implicit Q-Learning is an offline reinforcement learning algorithm that decouples value learning from policy improvement. It uses five neural networks: twin critics with parameters $theta_1$ and $theta_2$, their target versions with parameters $theta_1'$ and $theta_2'$, a value network with parameters $psi$, and a policy network with parameters $phi$. The expectile regression parameter $tau' in [0, 1]$ controls the conservatism in value estimation, while the temperature parameter $beta > 0$ determines how aggressively to exploit advantages. Experience tuples $(s, a, r, s')$ are stored in a replay buffer $cal(B)$ of capacity $N$, from which mini-batches of size $B$ are sampled. Networks are updated using learning rates $l_Q$, $l_V$, $l_pi$, and $l_alpha$ for critics, value network, policy, and temperature respectively, with target networks soft-updated at rate $tau$.

The first is a value network, which estimates the expected return from a state without considering actions. To update its weights, we compute the asymmetric $L_2$ loss:

$
L_V (psi) = EE_((s,a) ~ cal(B)) [L_2^(tau') (Q_theta (s, a) - V_psi (s))]
$

where $L_2^(tau') (u) = |tau' - bb(1) (u < 0)| . u^2$ is the asymmetric $L_2$ loss with parameter $tau'$.

The second is a Q-network, which estimates expected returns for state-action pairs. To update its weights, we compute the standard TD loss:

$
L_Q (theta_i) = EE_((s,a,r,s') ~ cal(B)) [(Q_(theta_i) (s, a) - r - gamma V_(psi) (s'))^2]
$

The last is a policy network that parameterizes a Gaussian distribution over actions. It outputs mean actions and learns log standard deviations as parameters. Using the reparameterization trick and tanh squashing for bounded actions, the policy is optimized by maximizing likelihood weighted by advantages:

$
L_(pi) (phi) = EE_((s,a) ~ cal(B)) \
[exp(beta . min(0, Q_(theta') (s, a) - V_(psi) (s))) . log pi_(phi) (a|s)]
$
where $beta$ controls the temperature and we clip advantages to be non-positive to avoid overoptimism.

=== Implementation detail of CAL-QL
Calibrated Q-learning extends offline RL with a conservative regularization approach. It uses three neural networks: a critic (Q-network) with parameters $psi$, its target version with parameters $psi'$, and a policy network with parameters $phi$. The temperature parameter $alpha > 0$ controls the conservative regularization strength, while the sampling parameter $n$ determines how many random and policy-sampled actions are used for regularization. The algorithm incorporates Monte Carlo returns from a dataset as reference values $V_mu (s)$ to mitigate overestimation. Experience tuples $(s, a, r, s')$ are stored in a replay buffer $cal(B)$ of capacity $N$, from which mini-batches of size $B$ are sampled. Networks are updated using learning rates $l_Q$, $l_\pi$, and $l_alpha$ for critic, policy, and temperature respectively, with target networks soft-updated at rate $tau$.

The first is a Q-network loss, which combines a standard TD loss with a conservative regularization term:

$
L_Q (psi) = L_Q^("TD") (psi) + alpha . L_Q^("CON") (psi)
$

The TD loss follows the standard form:
$
L_Q^("TD") (psi) = EE_((s,a,r,s') ~ cal(B)) \
[(Q_psi (s, a) - r - gamma Q_(psi') (s', a_(s')))^2]
$

The conservative regularization term penalizes Q-value overestimation:

$
L_Q^("CON") (psi)
= EE_(s ~ cal(B)) \
[log(
sum_(i=1)^n exp(Q_psi (s, a_R^i) - log(0.5^(|cal(A)|))) \
  + sum_(i=1)^n exp(max(V_mu (s), Q_psi (s, a_s^i) - log pi_phi (a_s^i|s)))) \
- Q_psi (s, a)]
$

where $a_R^i$ are random actions sampled uniformly and $V_mu (s)$ are reference values from Monte Carlo returns.

The policy network is optimized through the standard SAC policy loss:

$
L_pi (phi) = EE_(s ~ cal(B)) [alpha log(pi_phi (a_s | s)) - Q_psi (s, a_s)]
$
